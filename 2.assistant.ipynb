{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [공지]\n",
    "현재 하나의 Account / Organization을 여러 사용자가 사용하고 있습니다.    \n",
    "assistant, vector_store 생성 시 **이름 뒤에 자신을 식별할 수 있는 id를 추가해주세요.** (예. \"My Expense 01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assistant API Basic (v2)\n",
    "- assistant, thread, message, run을 생성할 수 있다. \n",
    "- run을 실행하고 결과 message를 출력할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "import json\n",
    "\n",
    "def show_json(obj):\n",
    "    \"\"\"객체의 정보를 보여줌\"\"\"\n",
    "    display(json.loads(obj.model_dump_json()))\n",
    "\n",
    "def pretty_print(messages):\n",
    "    \"\"\"메시지를 'role : text' 형태로 출력\"\"\"\n",
    "    print(\"# Messages\")\n",
    "    for m in messages:\n",
    "        print(f\"{m.role}: {m.content[0].text.value}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'asst_ai2uVPqsYq3IEtSCUr2xUhgj',\n",
       " 'created_at': 1713931592,\n",
       " 'description': None,\n",
       " 'instructions': '너는 복잡하고 어려운 IT 또는 컴퓨터 공학 지식을 비유를 사용해서 알기 쉽게 그리고 간결하게 설명해주는 나만의 IT 선생님',\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4-turbo',\n",
       " 'name': 'assistant ask',\n",
       " 'object': 'assistant',\n",
       " 'tools': [],\n",
       " 'response_format': 'auto',\n",
       " 'temperature': 1.0,\n",
       " 'tool_resources': {'code_interpreter': None, 'file_search': None},\n",
       " 'top_p': 1.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create assistant\n",
    "# Assistant 설명: 복잡하고 어려운 IT 또는 컴퓨터 공학 지식을 비유를 사용해서 알기 쉽게 그리고 간결하게 설명해주는 나만의 IT 선생님\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"assistant ask\", # 자신의 식별자 (id)를 이름 뒤에 붙여 주세요. \n",
    "    instructions=\"너는 복잡하고 어려운 IT 또는 컴퓨터 공학 지식을 비유를 사용해서 알기 쉽게 그리고 간결하게 설명해주는 나만의 IT 선생님\", # system message\n",
    "    model=\"gpt-4-turbo\", # gpt-4-turbo\n",
    ")\n",
    "show_json(assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'thread_oQnHTnC9XPRE8wSF4zHU1O76',\n",
       " 'created_at': 1713931647,\n",
       " 'metadata': {},\n",
       " 'object': 'thread',\n",
       " 'tool_resources': {'code_interpreter': None, 'file_search': None}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create empty threads\n",
    "thread = client.beta.threads.create()\n",
    "show_json(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'msg_Ho5IGpAouyxg9CzNEk4H2aBc',\n",
       " 'assistant_id': None,\n",
       " 'attachments': [],\n",
       " 'completed_at': None,\n",
       " 'content': [{'text': {'annotations': [],\n",
       "    'value': 'AI 모델이 너무 크기가 클 때 어떻게 해야 하지?'},\n",
       "   'type': 'text'}],\n",
       " 'created_at': 1713931962,\n",
       " 'incomplete_at': None,\n",
       " 'incomplete_details': None,\n",
       " 'metadata': {},\n",
       " 'object': 'thread.message',\n",
       " 'role': 'user',\n",
       " 'run_id': None,\n",
       " 'status': None,\n",
       " 'thread_id': 'thread_oQnHTnC9XPRE8wSF4zHU1O76'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create messages to thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id= thread.id, # 위에서 생성한 thread의 id 입력\n",
    "    role= 'user', # message의 role\n",
    "    content= 'AI 모델이 너무 크기가 클 때 어떻게 해야 하지?' # message의 content\n",
    ")\n",
    "show_json(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'run_EoXKYeG94FN80GqzXmEHJARo',\n",
       " 'assistant_id': 'asst_ai2uVPqsYq3IEtSCUr2xUhgj',\n",
       " 'cancelled_at': None,\n",
       " 'completed_at': None,\n",
       " 'created_at': 1713932052,\n",
       " 'expires_at': 1713932652,\n",
       " 'failed_at': None,\n",
       " 'incomplete_details': None,\n",
       " 'instructions': '너는 복잡하고 어려운 IT 또는 컴퓨터 공학 지식을 비유를 사용해서 알기 쉽게 그리고 간결하게 설명해주는 나만의 IT 선생님',\n",
       " 'last_error': None,\n",
       " 'max_completion_tokens': None,\n",
       " 'max_prompt_tokens': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4-turbo',\n",
       " 'object': 'thread.run',\n",
       " 'required_action': None,\n",
       " 'response_format': 'auto',\n",
       " 'started_at': None,\n",
       " 'status': 'queued',\n",
       " 'thread_id': 'thread_oQnHTnC9XPRE8wSF4zHU1O76',\n",
       " 'tool_choice': 'auto',\n",
       " 'tools': [],\n",
       " 'truncation_strategy': {'type': 'auto', 'last_messages': None},\n",
       " 'usage': None,\n",
       " 'temperature': 1.0,\n",
       " 'top_p': 1.0,\n",
       " 'tool_resources': {}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create run (thread와 assistant를 연결)\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id, # 위에서 생성한 thread의 id 입력\n",
    "    assistant_id=assistant.id # 위에서 생성한 assistant의 id 입력\n",
    ")\n",
    "show_json(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve run to check it's status\n",
    "import time\n",
    "def wait_on_run(run, thread):\n",
    "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id,\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wait_on_run(run, thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'run_EoXKYeG94FN80GqzXmEHJARo',\n",
       " 'assistant_id': 'asst_ai2uVPqsYq3IEtSCUr2xUhgj',\n",
       " 'cancelled_at': None,\n",
       " 'completed_at': 1713932068,\n",
       " 'created_at': 1713932052,\n",
       " 'expires_at': None,\n",
       " 'failed_at': None,\n",
       " 'incomplete_details': None,\n",
       " 'instructions': '너는 복잡하고 어려운 IT 또는 컴퓨터 공학 지식을 비유를 사용해서 알기 쉽게 그리고 간결하게 설명해주는 나만의 IT 선생님',\n",
       " 'last_error': None,\n",
       " 'max_completion_tokens': None,\n",
       " 'max_prompt_tokens': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4-turbo',\n",
       " 'object': 'thread.run',\n",
       " 'required_action': None,\n",
       " 'response_format': 'auto',\n",
       " 'started_at': 1713932053,\n",
       " 'status': 'completed',\n",
       " 'thread_id': 'thread_oQnHTnC9XPRE8wSF4zHU1O76',\n",
       " 'tool_choice': 'auto',\n",
       " 'tools': [],\n",
       " 'truncation_strategy': {'type': 'auto', 'last_messages': None},\n",
       " 'usage': {'completion_tokens': 429,\n",
       "  'prompt_tokens': 135,\n",
       "  'total_tokens': 564},\n",
       " 'temperature': 1.0,\n",
       " 'top_p': 1.0,\n",
       " 'tool_resources': {}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_json(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': 'msg_mAfqizvLU1B2HiisBFFlvhpf',\n",
       "   'assistant_id': None,\n",
       "   'attachments': [],\n",
       "   'completed_at': None,\n",
       "   'content': [{'text': {'annotations': [],\n",
       "      'value': 'LLM을 설명할 때 RAG를 같이 설명하는데 RAG에 대해서 설명해줘.'},\n",
       "     'type': 'text'}],\n",
       "   'created_at': 1713931781,\n",
       "   'incomplete_at': None,\n",
       "   'incomplete_details': None,\n",
       "   'metadata': {},\n",
       "   'object': 'thread.message',\n",
       "   'role': 'user',\n",
       "   'run_id': None,\n",
       "   'status': None,\n",
       "   'thread_id': 'thread_oQnHTnC9XPRE8wSF4zHU1O76'},\n",
       "  {'id': 'msg_Ho5IGpAouyxg9CzNEk4H2aBc',\n",
       "   'assistant_id': None,\n",
       "   'attachments': [],\n",
       "   'completed_at': None,\n",
       "   'content': [{'text': {'annotations': [],\n",
       "      'value': 'AI 모델이 너무 크기가 클 때 어떻게 해야 하지?'},\n",
       "     'type': 'text'}],\n",
       "   'created_at': 1713931962,\n",
       "   'incomplete_at': None,\n",
       "   'incomplete_details': None,\n",
       "   'metadata': {},\n",
       "   'object': 'thread.message',\n",
       "   'role': 'user',\n",
       "   'run_id': None,\n",
       "   'status': None,\n",
       "   'thread_id': 'thread_oQnHTnC9XPRE8wSF4zHU1O76'},\n",
       "  {'id': 'msg_MU9KiAuGugkSXvdShmhoIJgt',\n",
       "   'assistant_id': 'asst_ai2uVPqsYq3IEtSCUr2xUhgj',\n",
       "   'attachments': [],\n",
       "   'completed_at': None,\n",
       "   'content': [{'text': {'annotations': [],\n",
       "      'value': 'AI 모델이 너무 클 때는 마치 대형 트럭이 좁은 길을 지나가야 할 때처럼 어려움을 겪을 수 있습니다. 이 문제를 해결하는 방법 중 하나는 모델의 크기를 줄이는 것입니다. 이것을 모델 프루닝(model pruning)이라고 부르는데, 불필요한 부분을 절삭하여 더 작고 효율적인 모델을 만드는 과정입니다.\\n\\n또 다른 방법은 모델 파티셔닝(model partitioning)입니다. 이는 큰 모델을 여러 작은 조각으로 나누어 각각의 조각을 다른 서버나 프로세서에서 동시에 처리할 수 있도록 하는 기법이죠. 마치 큰 가구를 여러 조각으로 분해하여 좁은 문을 통해 이동시키는 것과 비슷합니다.\\n\\n마지막으로, 모델 양자화(model quantization)를 사용할 수 있습니다. 이는 모델이 사용하는 데이터의 정밀도를 낮추어 계산량과 메모리 사용량을 줄이는 기술입니다. 이 방법은 마치 고화질의 사진을 적절한 해상도로 줄여서 저장공간을 절약하는 것과 비슷하게 작동합니다.\\n\\n이러한 방법들을 통해 AI 모델의 크기를 관리하고, 효율적으로 운영할 수 있습니다.'},\n",
       "     'type': 'text'}],\n",
       "   'created_at': 1713932053,\n",
       "   'incomplete_at': None,\n",
       "   'incomplete_details': None,\n",
       "   'metadata': {},\n",
       "   'object': 'thread.message',\n",
       "   'role': 'assistant',\n",
       "   'run_id': 'run_EoXKYeG94FN80GqzXmEHJARo',\n",
       "   'status': None,\n",
       "   'thread_id': 'thread_oQnHTnC9XPRE8wSF4zHU1O76'}],\n",
       " 'object': 'list',\n",
       " 'first_id': 'msg_mAfqizvLU1B2HiisBFFlvhpf',\n",
       " 'last_id': 'msg_MU9KiAuGugkSXvdShmhoIJgt',\n",
       " 'has_more': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List messages in the thread \n",
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id = thread.id, # 위에서 생성한 thread의 id 입력, \n",
    "    order = \"asc\"\n",
    ")\n",
    "show_json(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message_run(assistant_id:str, thread_id:str, user_message:str):\n",
    "    \"\"\"\n",
    "    1. thread에 새로운 메시지를 추가하고 \n",
    "    2. run을 생성해 어시스턴트를 실행\n",
    "    \"\"\"\n",
    "    # 1. thread에 새로운 메시지를 추가하고 \n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id= thread_id, # 위에서 생성한 thread의 id 입력\n",
    "        role= 'user', # message의 role\n",
    "        content= user_message # message의 content\n",
    "    )\n",
    "    # 2. run을 생성해 어시스턴트를 실행\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread_id, # 위에서 생성한 thread의 id 입력\n",
    "        assistant_id=assistant_id # 위에서 생성한 assistant의 id 입력\n",
    "    )\n",
    "    \n",
    "    return run\n",
    "\n",
    "\n",
    "def get_response(thread_id):\n",
    "    \"\"\"\n",
    "    Thread의 메시지를 나열\n",
    "    \"\"\"\n",
    "    # Thread의 메시지를 나열 (생성일 기준 오름차순)\n",
    "    messages = client.beta.threads.messages.list(\n",
    "        thread_id = thread_id, # 위에서 생성한 thread의 id 입력, \n",
    "        order = \"asc\"\n",
    "    )\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Messages\n",
      "user: LLM을 설명할 때 RAG를 같이 설명하는데 RAG에 대해서 설명해줘.\n",
      "user: AI 모델이 너무 크기가 클 때 어떻게 해야 하지?\n",
      "assistant: AI 모델이 너무 클 때는 마치 대형 트럭이 좁은 길을 지나가야 할 때처럼 어려움을 겪을 수 있습니다. 이 문제를 해결하는 방법 중 하나는 모델의 크기를 줄이는 것입니다. 이것을 모델 프루닝(model pruning)이라고 부르는데, 불필요한 부분을 절삭하여 더 작고 효율적인 모델을 만드는 과정입니다.\n",
      "\n",
      "또 다른 방법은 모델 파티셔닝(model partitioning)입니다. 이는 큰 모델을 여러 작은 조각으로 나누어 각각의 조각을 다른 서버나 프로세서에서 동시에 처리할 수 있도록 하는 기법이죠. 마치 큰 가구를 여러 조각으로 분해하여 좁은 문을 통해 이동시키는 것과 비슷합니다.\n",
      "\n",
      "마지막으로, 모델 양자화(model quantization)를 사용할 수 있습니다. 이는 모델이 사용하는 데이터의 정밀도를 낮추어 계산량과 메모리 사용량을 줄이는 기술입니다. 이 방법은 마치 고화질의 사진을 적절한 해상도로 줄여서 저장공간을 절약하는 것과 비슷하게 작동합니다.\n",
      "\n",
      "이러한 방법들을 통해 AI 모델의 크기를 관리하고, 효율적으로 운영할 수 있습니다.\n",
      "user: 음... 그럼 모델 프로닝이 무엇이고 어떻게 하는 거야?\n",
      "assistant: 모델 프루닝(Model Pruning)이란, 인공지능 모델에서 중요도가 낮은 파라미터나 연결을 제거함으로써 모델의 크기를 줄이고, 처리 속도를 향상시키는 기법을 말해요. 즉, 네트워크 안에서 중요하지 않은 부분을 '가지치기'하는 것입니다.\n",
      "\n",
      "모델 프루닝을 하는 방법을 쉽게 설명드리자면, 마치 정원에서 건강하지 않거나 불필요한 가지를 잘라내 건강한 나무를 유지하는 것과 비슷합니다. 구체적으로는 다음과 같은 과정을 포함합니다:\n",
      "\n",
      "1. **중요도 평가**: 먼저, 모델의 각 가중치(weight)의 중요도를 평가합니다. 이는 그 가중치가 모델의 출력에 얼마나 큰 영향을 미치는지를 알아보기 위함입니다. 중요도는 가중치의 절대값, 오류율의 변화 등 다양한 방법으로 측정할 수 있습니다.\n",
      "\n",
      "2. **가중치 제거**: 중요도가 낮은 가중치들을 모델에서 제거합니다. 이는 그 가중치들이 결과에 크게 영향을 미치지 않는다고 판단되었기 때문입니다. \n",
      "\n",
      "3. **재학습**: 가중치를 제거한 후, 남은 가중치들로 모델을 다시 학습시켜 성능을 복원합니다. 이 과정은 제거된 가중치로 인한 성능 저하를 최소화하는 데 도움을 줍니다.\n",
      "\n",
      "4. **반복**: 필요에 따라 이 과정을 여러 번 반복하여 더 많은 가중치를 제거하고, 모델을 최적화할 수 있습니다.\n",
      "\n",
      "모델 프루닝은 모델의 크기와 복잡성을 줄이면서도 성능을 유지하려는 경우에 유용하게 사용될 수 있습니다. 이는 특히 모바일이나 임베디드 디바이스와 같이 컴퓨팅 자원이 제한된 환경에서 매우 중요할 수 있어요.\n",
      "user: 그러면 그 방법을 사용해서 모델을 가볍게 만들면 뭐가 좋아?\n",
      "assistant: 모델을 가볍게 만드는 것, 즉 모델 프루닝 같은 기법을 사용하는 이유는 여러 가지가 있습니다. 여기 주요 이점들을 몇 가지 설명해 드리겠습니다:\n",
      "\n",
      "1. **속도 향상**: 모델의 크기가 작아지면 계산해야 할 양이 줄어들어 처리 속도가 빨라집니다. 이는 사용자 경험을 향상시킬 뿐만 아니라, 실시간 처리가 중요한 애플리케이션에서 특히 중요할 수 있습니다.\n",
      "\n",
      "2. **저장 공간 절약**: 더 작은 모델은 더 적은 저장 공간을 차지합니다. 이는 특히 저장 공간이 제한적인 모바일 기기나 임베디드 시스템에서 유리합니다.\n",
      "\n",
      "3. **에너지 효율성 증가**: 더 적은 계산은 더 적은 전력 소비를 의미합니다. 이는 배터리로 작동하는 장치에서 특히 중요할 수 있고, 대규모 데이터 센터에서는 에너지 비용을 절감할 수 있습니다.\n",
      "\n",
      "4. **배포 용이성**: 가벼운 모델은 배포가 더 간단하고, 업데이트가 쉽습니다. 네트워크 대역폭이 제한적인 환경에서는 더 작은 모델을 더 쉽게 전송할 수 있습니다.\n",
      "\n",
      "5. **범용성**: 가벼운 모델은 다양한 플랫폼과 장치에 적용하기 쉽습니다. 개발자는 특정 하드웨어에 맞춰 최적화하지 않고도 여러 가지 환경에서 모델을 사용할 수 있습니다.\n",
      "\n",
      "종합하면, 모델을 가볍게 만드는 기법은 효율성, 접근성 및 경제성을 높여 주어, AI 기술을 더 넓은 범위의 애플리케이션과 환경에 적용할 수 있게 돕습니다. 특히 클라우드 컴퓨팅이 아닌 엣지 컴퓨팅 환경에서는 이러한 이점이 더욱 도드라질 수 있습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "TECH_ASSISTANT_ID = assistant.id\n",
    "CUR_THREAD_ID = thread.id\n",
    "\n",
    "# user_message = \"음... 그럼 모델 프루닝이 무엇이고 어떻게 하는 거야?\" # 새로운 메시지 입력\n",
    "user_message = \"그러면 그 방법을 사용해서 모델을 가볍게 만들면 뭐가 좋아?\" # 새로운 메시지 입력\n",
    "run = add_message_run(TECH_ASSISTANT_ID, CUR_THREAD_ID, user_message)\n",
    "run = wait_on_run(run, thread) \n",
    "messages = get_response(CUR_THREAD_ID)\n",
    "pretty_print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assistant API Advanced\n",
    "- Assistant를 업데이트 할 수 있다.\n",
    "    - `VectorStore` 생성\n",
    "    - `tools`에 `file_search` 추가\n",
    "- [추가] annotation을 표시할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the files (SKIP)\n",
    "file = client.files.create(\n",
    "    file=open(\n",
    "        \"./data/coffeechat1.txt\",\n",
    "        \"rb\",\n",
    "    ),\n",
    "    purpose=\"assistants\",\n",
    ")\n",
    "\n",
    "file = client.files.create(\n",
    "    file=open(\n",
    "        \"./data/coffeechat2.txt\",\n",
    "        \"rb\",\n",
    "    ),\n",
    "    purpose=\"assistants\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FileObject(id='file-StNE32pHiVzosq3eY9PbYjfw', bytes=11217, created_at=1713933656, filename='coffeechat2.txt', object='file', purpose='assistants', status='processed', status_details=None),\n",
       " FileObject(id='file-IkfTcOBo6mU1oVbGqzaLMjUt', bytes=18742, created_at=1713933655, filename='coffeechat1.txt', object='file', purpose='assistants', status='processed', status_details=None),\n",
       " FileObject(id='file-dWqc4HtyKMfneijZaGtc3A9B', bytes=11217, created_at=1713933635, filename='coffeechat2.txt', object='file', purpose='assistants', status='processed', status_details=None),\n",
       " FileObject(id='file-TNpKiB4sCig4yg9MAAOes0ym', bytes=18742, created_at=1713933634, filename='coffeechat1.txt', object='file', purpose='assistants', status='processed', status_details=None),\n",
       " FileObject(id='file-jRGtDpy9tHBFQS6r2nsoyAGr', bytes=11217, created_at=1713933629, filename='coffeechat2.txt', object='file', purpose='assistants', status='processed', status_details=None),\n",
       " FileObject(id='file-wetyz5mML28Bmxp7sT0RCXyH', bytes=18742, created_at=1713933628, filename='coffeechat1.txt', object='file', purpose='assistants', status='processed', status_details=None),\n",
       " FileObject(id='file-nAs3c8ibHnzJSyrRsvXlbe2r', bytes=11217, created_at=1713933593, filename='coffeechat2.txt', object='file', purpose='assistants', status='processed', status_details=None),\n",
       " FileObject(id='file-OpvPi0WPe9fFL48IX4xjLDQU', bytes=18742, created_at=1713933593, filename='coffeechat1.txt', object='file', purpose='assistants', status='processed', status_details=None),\n",
       " FileObject(id='file-w4jY5bTblb8GgJEzCC58QpxE', bytes=11217, created_at=1713933540, filename='coffeechat2.txt', object='file', purpose='assistants', status='processed', status_details=None),\n",
       " FileObject(id='file-xVxsyneVPP5G8wBjG8Fp2GfR', bytes=18742, created_at=1713933539, filename='coffeechat1.txt', object='file', purpose='assistants', status='processed', status_details=None),\n",
       " FileObject(id='file-NPT59yxwKHWWvMYFpBEMg8lh', bytes=11217, created_at=1713933235, filename='coffeechat2.txt', object='file', purpose='assistants', status='processed', status_details=None),\n",
       " FileObject(id='file-ZvSBuyKcICYfMYlMV1PeAiJj', bytes=18742, created_at=1713933234, filename='coffeechat1.txt', object='file', purpose='assistants', status='processed', status_details=None)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.files.list().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'file-StNE32pHiVzosq3eY9PbYjfw',\n",
       " 'bytes': 11217,\n",
       " 'created_at': 1713933656,\n",
       " 'filename': 'coffeechat2.txt',\n",
       " 'object': 'file',\n",
       " 'purpose': 'assistants',\n",
       " 'status': 'processed',\n",
       " 'status_details': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'file-IkfTcOBo6mU1oVbGqzaLMjUt',\n",
       " 'bytes': 18742,\n",
       " 'created_at': 1713933655,\n",
       " 'filename': 'coffeechat1.txt',\n",
       " 'object': 'file',\n",
       " 'purpose': 'assistants',\n",
       " 'status': 'processed',\n",
       " 'status_details': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'file-dWqc4HtyKMfneijZaGtc3A9B',\n",
       " 'bytes': 11217,\n",
       " 'created_at': 1713933635,\n",
       " 'filename': 'coffeechat2.txt',\n",
       " 'object': 'file',\n",
       " 'purpose': 'assistants',\n",
       " 'status': 'processed',\n",
       " 'status_details': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'file-TNpKiB4sCig4yg9MAAOes0ym',\n",
       " 'bytes': 18742,\n",
       " 'created_at': 1713933634,\n",
       " 'filename': 'coffeechat1.txt',\n",
       " 'object': 'file',\n",
       " 'purpose': 'assistants',\n",
       " 'status': 'processed',\n",
       " 'status_details': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'file-jRGtDpy9tHBFQS6r2nsoyAGr',\n",
       " 'bytes': 11217,\n",
       " 'created_at': 1713933629,\n",
       " 'filename': 'coffeechat2.txt',\n",
       " 'object': 'file',\n",
       " 'purpose': 'assistants',\n",
       " 'status': 'processed',\n",
       " 'status_details': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'file-wetyz5mML28Bmxp7sT0RCXyH',\n",
       " 'bytes': 18742,\n",
       " 'created_at': 1713933628,\n",
       " 'filename': 'coffeechat1.txt',\n",
       " 'object': 'file',\n",
       " 'purpose': 'assistants',\n",
       " 'status': 'processed',\n",
       " 'status_details': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'file-nAs3c8ibHnzJSyrRsvXlbe2r',\n",
       " 'bytes': 11217,\n",
       " 'created_at': 1713933593,\n",
       " 'filename': 'coffeechat2.txt',\n",
       " 'object': 'file',\n",
       " 'purpose': 'assistants',\n",
       " 'status': 'processed',\n",
       " 'status_details': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'file-OpvPi0WPe9fFL48IX4xjLDQU',\n",
       " 'bytes': 18742,\n",
       " 'created_at': 1713933593,\n",
       " 'filename': 'coffeechat1.txt',\n",
       " 'object': 'file',\n",
       " 'purpose': 'assistants',\n",
       " 'status': 'processed',\n",
       " 'status_details': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'file-w4jY5bTblb8GgJEzCC58QpxE',\n",
       " 'bytes': 11217,\n",
       " 'created_at': 1713933540,\n",
       " 'filename': 'coffeechat2.txt',\n",
       " 'object': 'file',\n",
       " 'purpose': 'assistants',\n",
       " 'status': 'processed',\n",
       " 'status_details': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'file-xVxsyneVPP5G8wBjG8Fp2GfR',\n",
       " 'bytes': 18742,\n",
       " 'created_at': 1713933539,\n",
       " 'filename': 'coffeechat1.txt',\n",
       " 'object': 'file',\n",
       " 'purpose': 'assistants',\n",
       " 'status': 'processed',\n",
       " 'status_details': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'file-NPT59yxwKHWWvMYFpBEMg8lh',\n",
       " 'bytes': 11217,\n",
       " 'created_at': 1713933235,\n",
       " 'filename': 'coffeechat2.txt',\n",
       " 'object': 'file',\n",
       " 'purpose': 'assistants',\n",
       " 'status': 'processed',\n",
       " 'status_details': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'file-ZvSBuyKcICYfMYlMV1PeAiJj',\n",
       " 'bytes': 18742,\n",
       " 'created_at': 1713933234,\n",
       " 'filename': 'coffeechat1.txt',\n",
       " 'object': 'file',\n",
       " 'purpose': 'assistants',\n",
       " 'status': 'processed',\n",
       " 'status_details': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view files\n",
    "for file_obj in client.files.list(purpose=\"assistants\").data:\n",
    "    show_json(file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'vs_ZjiJs9RLtMp6eVifE4tZRoc6',\n",
       " 'bytes': None,\n",
       " 'created_at': 1713933977,\n",
       " 'file_counts': {'cancelled': 0,\n",
       "  'completed': 2,\n",
       "  'failed': 0,\n",
       "  'in_progress': 0,\n",
       "  'total': 2},\n",
       " 'last_active_at': 1713933977,\n",
       " 'metadata': {},\n",
       " 'name': 'Quantization Coffee Chat - ahnsk',\n",
       " 'object': 'vector_store',\n",
       " 'status': 'completed',\n",
       " 'expires_after': None,\n",
       " 'expires_at': None,\n",
       " 'usage_bytes': 37451}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vector store 생성\n",
    "vector_store = client.beta.vector_stores.create(\n",
    "  name=\"Quantization Coffee Chat - ahnsk\",\n",
    "  #file_ids=[file_obj.id for file_obj in client.files.list(purpose=\"assistants\").data],\n",
    "  file_ids=[\"file-ZvSBuyKcICYfMYlMV1PeAiJj\", \"file-NPT59yxwKHWWvMYFpBEMg8lh\"], # coffeechat1.txt 과 coffeechat2.txt \n",
    ")\n",
    "show_json(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'asst_ai2uVPqsYq3IEtSCUr2xUhgj',\n",
       " 'created_at': 1713931592,\n",
       " 'description': None,\n",
       " 'instructions': '너는 복잡하고 어려운 IT 또는 컴퓨터 공학 지식을 비유를 사용해서 알기 쉽게 그리고 간결하게 설명해주는 나만의 IT 선생님',\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4-turbo',\n",
       " 'name': 'assistant ask',\n",
       " 'object': 'assistant',\n",
       " 'tools': [{'type': 'file_search'}],\n",
       " 'response_format': 'auto',\n",
       " 'temperature': 1.0,\n",
       " 'tool_resources': {'code_interpreter': None,\n",
       "  'file_search': {'vector_store_ids': ['vs_ZjiJs9RLtMp6eVifE4tZRoc6']}},\n",
       " 'top_p': 1.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Update Assistant: file_search 기능을 추가\n",
    "assistant = client.beta.assistants.update(\n",
    "    assistant_id = TECH_ASSISTANT_ID, # assistant ID\n",
    "    tools = [{\"type\": \"file_search\"}], # file_search 기능을 추가\n",
    "    tool_resources = {\"file_search\": {\n",
    "            \"vector_store_ids\": [vector_store.id]\n",
    "        }\n",
    "    } # tools_resource에 vectorDB를 추가\n",
    ")\n",
    "show_json(assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Messages\n",
      "user: LLM을 설명할 때 RAG를 같이 설명하는데 RAG에 대해서 설명해줘.\n",
      "user: AI 모델이 너무 크기가 클 때 어떻게 해야 하지?\n",
      "assistant: AI 모델이 너무 클 때는 마치 대형 트럭이 좁은 길을 지나가야 할 때처럼 어려움을 겪을 수 있습니다. 이 문제를 해결하는 방법 중 하나는 모델의 크기를 줄이는 것입니다. 이것을 모델 프루닝(model pruning)이라고 부르는데, 불필요한 부분을 절삭하여 더 작고 효율적인 모델을 만드는 과정입니다.\n",
      "\n",
      "또 다른 방법은 모델 파티셔닝(model partitioning)입니다. 이는 큰 모델을 여러 작은 조각으로 나누어 각각의 조각을 다른 서버나 프로세서에서 동시에 처리할 수 있도록 하는 기법이죠. 마치 큰 가구를 여러 조각으로 분해하여 좁은 문을 통해 이동시키는 것과 비슷합니다.\n",
      "\n",
      "마지막으로, 모델 양자화(model quantization)를 사용할 수 있습니다. 이는 모델이 사용하는 데이터의 정밀도를 낮추어 계산량과 메모리 사용량을 줄이는 기술입니다. 이 방법은 마치 고화질의 사진을 적절한 해상도로 줄여서 저장공간을 절약하는 것과 비슷하게 작동합니다.\n",
      "\n",
      "이러한 방법들을 통해 AI 모델의 크기를 관리하고, 효율적으로 운영할 수 있습니다.\n",
      "user: 음... 그럼 모델 프로닝이 무엇이고 어떻게 하는 거야?\n",
      "assistant: 모델 프루닝(Model Pruning)이란, 인공지능 모델에서 중요도가 낮은 파라미터나 연결을 제거함으로써 모델의 크기를 줄이고, 처리 속도를 향상시키는 기법을 말해요. 즉, 네트워크 안에서 중요하지 않은 부분을 '가지치기'하는 것입니다.\n",
      "\n",
      "모델 프루닝을 하는 방법을 쉽게 설명드리자면, 마치 정원에서 건강하지 않거나 불필요한 가지를 잘라내 건강한 나무를 유지하는 것과 비슷합니다. 구체적으로는 다음과 같은 과정을 포함합니다:\n",
      "\n",
      "1. **중요도 평가**: 먼저, 모델의 각 가중치(weight)의 중요도를 평가합니다. 이는 그 가중치가 모델의 출력에 얼마나 큰 영향을 미치는지를 알아보기 위함입니다. 중요도는 가중치의 절대값, 오류율의 변화 등 다양한 방법으로 측정할 수 있습니다.\n",
      "\n",
      "2. **가중치 제거**: 중요도가 낮은 가중치들을 모델에서 제거합니다. 이는 그 가중치들이 결과에 크게 영향을 미치지 않는다고 판단되었기 때문입니다. \n",
      "\n",
      "3. **재학습**: 가중치를 제거한 후, 남은 가중치들로 모델을 다시 학습시켜 성능을 복원합니다. 이 과정은 제거된 가중치로 인한 성능 저하를 최소화하는 데 도움을 줍니다.\n",
      "\n",
      "4. **반복**: 필요에 따라 이 과정을 여러 번 반복하여 더 많은 가중치를 제거하고, 모델을 최적화할 수 있습니다.\n",
      "\n",
      "모델 프루닝은 모델의 크기와 복잡성을 줄이면서도 성능을 유지하려는 경우에 유용하게 사용될 수 있습니다. 이는 특히 모바일이나 임베디드 디바이스와 같이 컴퓨팅 자원이 제한된 환경에서 매우 중요할 수 있어요.\n",
      "user: 그러면 그 방법을 사용해서 모델을 가볍게 만들면 뭐가 좋아?\n",
      "assistant: 모델을 가볍게 만드는 것, 즉 모델 프루닝 같은 기법을 사용하는 이유는 여러 가지가 있습니다. 여기 주요 이점들을 몇 가지 설명해 드리겠습니다:\n",
      "\n",
      "1. **속도 향상**: 모델의 크기가 작아지면 계산해야 할 양이 줄어들어 처리 속도가 빨라집니다. 이는 사용자 경험을 향상시킬 뿐만 아니라, 실시간 처리가 중요한 애플리케이션에서 특히 중요할 수 있습니다.\n",
      "\n",
      "2. **저장 공간 절약**: 더 작은 모델은 더 적은 저장 공간을 차지합니다. 이는 특히 저장 공간이 제한적인 모바일 기기나 임베디드 시스템에서 유리합니다.\n",
      "\n",
      "3. **에너지 효율성 증가**: 더 적은 계산은 더 적은 전력 소비를 의미합니다. 이는 배터리로 작동하는 장치에서 특히 중요할 수 있고, 대규모 데이터 센터에서는 에너지 비용을 절감할 수 있습니다.\n",
      "\n",
      "4. **배포 용이성**: 가벼운 모델은 배포가 더 간단하고, 업데이트가 쉽습니다. 네트워크 대역폭이 제한적인 환경에서는 더 작은 모델을 더 쉽게 전송할 수 있습니다.\n",
      "\n",
      "5. **범용성**: 가벼운 모델은 다양한 플랫폼과 장치에 적용하기 쉽습니다. 개발자는 특정 하드웨어에 맞춰 최적화하지 않고도 여러 가지 환경에서 모델을 사용할 수 있습니다.\n",
      "\n",
      "종합하면, 모델을 가볍게 만드는 기법은 효율성, 접근성 및 경제성을 높여 주어, AI 기술을 더 넓은 범위의 애플리케이션과 환경에 적용할 수 있게 돕습니다. 특히 클라우드 컴퓨팅이 아닌 엣지 컴퓨팅 환경에서는 이러한 이점이 더욱 도드라질 수 있습니다.\n",
      "user: 인터뷰에서 가장 중요하게 다룬 경량화 종류는 무엇이야?\n",
      "assistant: 인터뷰에서 가장 중요하게 다룬 경량화 방식은 \"Pruning\"과 \"Knowledge Distillation\"입니다.\n",
      "\n",
      "1. **Pruning**: 모델에서 중요하지 않은 부분(파라미터, 가중치 등)을 잘라내서 모델의 크기를 줄이는 기법입니다. 이 방법은 전체 모델의 중요하지 않은 부분을 제거하여 경량화하므로 모델의 성능에 큰 영향을 주지 않으면서 크기를 줄일 수 있습니다 .\n",
      "\n",
      "2. **Knowledge Distillation**: 더 크고 성능이 좋은 모델의 지식을 이용하여 더 작은 모델을 학습시키는 방법입니다. 이 방법을 활용하면 대형 모델의 핵심적인 정보만을 작은 모델로 전달해 성능은 유지하면서 크기는 줄일 수 있습니다【11:0†source】.\n",
      "\n",
      "이 두 방법은 모두 모델의 크기와 계산 요구량을 줄이는 데에 초점을 맞추고 있으며, 이는 인터뷰에서 언급된 주요 경량화 방식입니다.\n",
      "user: 이번 인터뷰에 어떤 사람이 나와서 무슨 이야기해?\n",
      "assistant: 이번 인터뷰에는 김태수 씨가 나와서 주로 모델 경량화에 대해 이야기했습니다. 그는 KAIST와 포항공과대학교(POSTECH)에서 공부하고 현재 SqueezeBits라는 회사에서 일하고 있으며, 그의 전문 분야는 AI 모델의 경량화입니다. 인터뷰에서 그는 모델 경량화의 중요성을 강조하며 이 기술이 어떻게 AI 모델을 더 작고 효율적으로 만드는지 설명했습니다. 주요 논의 주제는 Pruning과 Knowledge Distillation이었으며, 이 기술들을 통해 고성능 모델의 지식을 더 작은 모델에 전달하는 방법에 대해 토론되었습니다 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# user_message = \"인터뷰에서 가장 중요하게 다룬 경량화 종류는 무엇이야?\" # 새로운 메시지 입력 (\"인터뷰에서 가장 중요하게 다룬 경량화 종류는 무엇이야?\")\n",
    "user_message = \"이번 인터뷰에 어떤 사람이 나와서 무슨 이야기해?\"\n",
    "run = add_message_run(TECH_ASSISTANT_ID, CUR_THREAD_ID, user_message) # 메시지를 보내고 run을 생성\n",
    "run = wait_on_run(run, thread) # run 완료까지 대기\n",
    "messages = get_response(CUR_THREAD_ID) # 결과 확인\n",
    "pretty_print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextContentBlock(text=Text(annotations=[], value='이번 인터뷰에는 김태수 씨가 나와서 주로 모델 경량화에 대해 이야기했습니다. 그는 KAIST와 포항공과대학교(POSTECH)에서 공부하고 현재 SqueezeBits라는 회사에서 일하고 있으며, 그의 전문 분야는 AI 모델의 경량화입니다. 인터뷰에서 그는 모델 경량화의 중요성을 강조하며 이 기술이 어떻게 AI 모델을 더 작고 효율적으로 만드는지 설명했습니다. 주요 논의 주제는 Pruning과 Knowledge Distillation이었으며, 이 기술들을 통해 고성능 모델의 지식을 더 작은 모델에 전달하는 방법에 대해 토론되었습니다 .'), type='text')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.data[-1].content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(annotations=[], value='이번 인터뷰에는 김태수 씨가 나와서 주로 모델 경량화에 대해 이야기했습니다. 그는 KAIST와 포항공과대학교(POSTECH)에서 공부하고 현재 SqueezeBits라는 회사에서 일하고 있으며, 그의 전문 분야는 AI 모델의 경량화입니다. 인터뷰에서 그는 모델 경량화의 중요성을 강조하며 이 기술이 어떻게 AI 모델을 더 작고 효율적으로 만드는지 설명했습니다. 주요 논의 주제는 Pruning과 Knowledge Distillation이었으며, 이 기술들을 통해 고성능 모델의 지식을 더 작은 모델에 전달하는 방법에 대해 토론되었습니다 .')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [참고] Retrieval의 경우 annotation (어떤 파일을 참고했는지) 정보를 확인할 수 있음\n",
    "messages.data[-1].content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [참고] annotation의 활용: footprint(ex. [0], [1], ...)로 표현하기\n",
    "message_content = messages.data[-1].content[0].text\n",
    "annotations = messages.data[-1].content[0].text.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    # 텍스트에서 annotation을 footnote로 대체\n",
    "    message_content.value = message_content.value.replace(annotation.text, f' [{index}]')\n",
    "\n",
    "    if getattr(annotation, 'file_citation', None):\n",
    "        file_citation = getattr(annotation, 'file_citation', None)\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f'[{index}] {file_citation.quote} from {cited_file.filename}')\n",
    "\n",
    "# Add footnotes to the end of the message before displaying to user\n",
    "message_content.value += '\\n' + '\\n'.join(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Messages\n",
      "user: LLM을 설명할 때 RAG를 같이 설명하는데 RAG에 대해서 설명해줘.\n",
      "user: AI 모델이 너무 크기가 클 때 어떻게 해야 하지?\n",
      "assistant: AI 모델이 너무 클 때는 마치 대형 트럭이 좁은 길을 지나가야 할 때처럼 어려움을 겪을 수 있습니다. 이 문제를 해결하는 방법 중 하나는 모델의 크기를 줄이는 것입니다. 이것을 모델 프루닝(model pruning)이라고 부르는데, 불필요한 부분을 절삭하여 더 작고 효율적인 모델을 만드는 과정입니다.\n",
      "\n",
      "또 다른 방법은 모델 파티셔닝(model partitioning)입니다. 이는 큰 모델을 여러 작은 조각으로 나누어 각각의 조각을 다른 서버나 프로세서에서 동시에 처리할 수 있도록 하는 기법이죠. 마치 큰 가구를 여러 조각으로 분해하여 좁은 문을 통해 이동시키는 것과 비슷합니다.\n",
      "\n",
      "마지막으로, 모델 양자화(model quantization)를 사용할 수 있습니다. 이는 모델이 사용하는 데이터의 정밀도를 낮추어 계산량과 메모리 사용량을 줄이는 기술입니다. 이 방법은 마치 고화질의 사진을 적절한 해상도로 줄여서 저장공간을 절약하는 것과 비슷하게 작동합니다.\n",
      "\n",
      "이러한 방법들을 통해 AI 모델의 크기를 관리하고, 효율적으로 운영할 수 있습니다.\n",
      "user: 음... 그럼 모델 프로닝이 무엇이고 어떻게 하는 거야?\n",
      "assistant: 모델 프루닝(Model Pruning)이란, 인공지능 모델에서 중요도가 낮은 파라미터나 연결을 제거함으로써 모델의 크기를 줄이고, 처리 속도를 향상시키는 기법을 말해요. 즉, 네트워크 안에서 중요하지 않은 부분을 '가지치기'하는 것입니다.\n",
      "\n",
      "모델 프루닝을 하는 방법을 쉽게 설명드리자면, 마치 정원에서 건강하지 않거나 불필요한 가지를 잘라내 건강한 나무를 유지하는 것과 비슷합니다. 구체적으로는 다음과 같은 과정을 포함합니다:\n",
      "\n",
      "1. **중요도 평가**: 먼저, 모델의 각 가중치(weight)의 중요도를 평가합니다. 이는 그 가중치가 모델의 출력에 얼마나 큰 영향을 미치는지를 알아보기 위함입니다. 중요도는 가중치의 절대값, 오류율의 변화 등 다양한 방법으로 측정할 수 있습니다.\n",
      "\n",
      "2. **가중치 제거**: 중요도가 낮은 가중치들을 모델에서 제거합니다. 이는 그 가중치들이 결과에 크게 영향을 미치지 않는다고 판단되었기 때문입니다. \n",
      "\n",
      "3. **재학습**: 가중치를 제거한 후, 남은 가중치들로 모델을 다시 학습시켜 성능을 복원합니다. 이 과정은 제거된 가중치로 인한 성능 저하를 최소화하는 데 도움을 줍니다.\n",
      "\n",
      "4. **반복**: 필요에 따라 이 과정을 여러 번 반복하여 더 많은 가중치를 제거하고, 모델을 최적화할 수 있습니다.\n",
      "\n",
      "모델 프루닝은 모델의 크기와 복잡성을 줄이면서도 성능을 유지하려는 경우에 유용하게 사용될 수 있습니다. 이는 특히 모바일이나 임베디드 디바이스와 같이 컴퓨팅 자원이 제한된 환경에서 매우 중요할 수 있어요.\n",
      "user: 그러면 그 방법을 사용해서 모델을 가볍게 만들면 뭐가 좋아?\n",
      "assistant: 모델을 가볍게 만드는 것, 즉 모델 프루닝 같은 기법을 사용하는 이유는 여러 가지가 있습니다. 여기 주요 이점들을 몇 가지 설명해 드리겠습니다:\n",
      "\n",
      "1. **속도 향상**: 모델의 크기가 작아지면 계산해야 할 양이 줄어들어 처리 속도가 빨라집니다. 이는 사용자 경험을 향상시킬 뿐만 아니라, 실시간 처리가 중요한 애플리케이션에서 특히 중요할 수 있습니다.\n",
      "\n",
      "2. **저장 공간 절약**: 더 작은 모델은 더 적은 저장 공간을 차지합니다. 이는 특히 저장 공간이 제한적인 모바일 기기나 임베디드 시스템에서 유리합니다.\n",
      "\n",
      "3. **에너지 효율성 증가**: 더 적은 계산은 더 적은 전력 소비를 의미합니다. 이는 배터리로 작동하는 장치에서 특히 중요할 수 있고, 대규모 데이터 센터에서는 에너지 비용을 절감할 수 있습니다.\n",
      "\n",
      "4. **배포 용이성**: 가벼운 모델은 배포가 더 간단하고, 업데이트가 쉽습니다. 네트워크 대역폭이 제한적인 환경에서는 더 작은 모델을 더 쉽게 전송할 수 있습니다.\n",
      "\n",
      "5. **범용성**: 가벼운 모델은 다양한 플랫폼과 장치에 적용하기 쉽습니다. 개발자는 특정 하드웨어에 맞춰 최적화하지 않고도 여러 가지 환경에서 모델을 사용할 수 있습니다.\n",
      "\n",
      "종합하면, 모델을 가볍게 만드는 기법은 효율성, 접근성 및 경제성을 높여 주어, AI 기술을 더 넓은 범위의 애플리케이션과 환경에 적용할 수 있게 돕습니다. 특히 클라우드 컴퓨팅이 아닌 엣지 컴퓨팅 환경에서는 이러한 이점이 더욱 도드라질 수 있습니다.\n",
      "user: 인터뷰에서 가장 중요하게 다룬 경량화 종류는 무엇이야?\n",
      "assistant: 인터뷰에서 가장 중요하게 다룬 경량화 방식은 \"Pruning\"과 \"Knowledge Distillation\"입니다.\n",
      "\n",
      "1. **Pruning**: 모델에서 중요하지 않은 부분(파라미터, 가중치 등)을 잘라내서 모델의 크기를 줄이는 기법입니다. 이 방법은 전체 모델의 중요하지 않은 부분을 제거하여 경량화하므로 모델의 성능에 큰 영향을 주지 않으면서 크기를 줄일 수 있습니다 .\n",
      "\n",
      "2. **Knowledge Distillation**: 더 크고 성능이 좋은 모델의 지식을 이용하여 더 작은 모델을 학습시키는 방법입니다. 이 방법을 활용하면 대형 모델의 핵심적인 정보만을 작은 모델로 전달해 성능은 유지하면서 크기는 줄일 수 있습니다【11:0†source】.\n",
      "\n",
      "이 두 방법은 모두 모델의 크기와 계산 요구량을 줄이는 데에 초점을 맞추고 있으며, 이는 인터뷰에서 언급된 주요 경량화 방식입니다.\n",
      "user: 이번 인터뷰에 어떤 사람이 나와서 무슨 이야기해?\n",
      "assistant: 이번 인터뷰에는 김태수 씨가 나와서 주로 모델 경량화에 대해 이야기했습니다. 그는 KAIST와 포항공과대학교(POSTECH)에서 공부하고 현재 SqueezeBits라는 회사에서 일하고 있으며, 그의 전문 분야는 AI 모델의 경량화입니다. 인터뷰에서 그는 모델 경량화의 중요성을 강조하며 이 기술이 어떻게 AI 모델을 더 작고 효율적으로 만드는지 설명했습니다. 주요 논의 주제는 Pruning과 Knowledge Distillation이었으며, 이 기술들을 통해 고성능 모델의 지식을 더 작은 모델에 전달하는 방법에 대해 토론되었습니다 .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pretty_print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [도전] 나만의 Finance Assistant 생성\n",
    "- Finance Assistant 설명\n",
    "    - 나의 지출 데이터를 보고 지출 상태를 시각화 해줍니다. (그래프 생성)\n",
    "    - 나의 지출 데이터 기반으로 전문적인 지출 관리 조언을 제공해줍니다.\n",
    "- Assistant의 tools에 \"file_search\"와 \"code_interpreter\" 기능을 추가합니다.\n",
    "- \"expenses.md\" 파일을 업로드해 vectore store를 생성하고 이를 tools_resource에 입력합니다.\n",
    "- \"나의 지출 상태를 바 차트로 시각화 해줘\"라는 메시지를 보내고 응답을 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (openapi-api)",
   "language": "python",
   "name": "openapi-api"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
