업계에서 모델 다이어트로 진짜 유명하시잖아요
커피챗 수락해 주셔서 정말 감사해요
SqueezeBits에서 일하고 있는 김태수 입니다
경량화에 질문드리기 앞서서 태수님 이력이 너무 화려해요 무려 포카
현재 공동 창업까지 하셔 가지고 회사를 이끌고 계세요
대학교 때 어떤 연구를 하셨는지
스토리도 너무 궁금해요 시작부터 자기자랑 하기가
좀 그런데 포카를 다 다녔구요 학부때는
카이스트에서 먼저 있었고
카이스트 전기및전자공학부 일거에요
컴퓨터로 low 레벨에서 일어나는 일들에 관심이 많아 가지고
대표적으로 생각나는 과목들을 컴퓨터아키텍처 CPU가 어떻게 작동하는지
그리고 어셈블리어나 컴파일러가 어떻게 작동하는지
그런 거를 배웠고요
정보이론 정도가 있는 것 같아요
정보이론은 INFORMATION채널이란 게 있을 때
정보가 그 채널을 통해서 전달될 때
이제 어떻게 압축을 해야 되고
손실이 나면 어떻게 복구해야 되는지 다룬 과목 이거든요
이게 Neural Network 에서도 비슷하게 사용이 되고 있어서
그 정도가 생각이 나는 거 같습니다
그래서 개인적으로 학부수업 때 배웠던 거
되게 아직도 다 해서 먹고 있고요
카이스트에서 포항공대 가는 경우가 흔치는 않아요
4학년 여름방학에 친구 손에 이끌려서
그 친구는 포스텍 다니던 친구였거든요
지도교수님 랩에서 연구 참여를 한번 하게 됐습니다
거기서 학부연구 참여를 했더니
교수님이 너무 좋으셔 주제도 너무 좋고
포항으로 박사 수련을 떠나서 그래서 포스텍에서는
이제 IT 융합공학과라는 과에 있었는데
저희 교수님은 칩설계 전공을 하신 분이셔서
Neural Network 연산에 최적화된 NPU 아키텍처 설계를 연구했고
그러다 보니까
이제 자연스럽게 뉴럴 네트워크 모델이 엄청 크잖아요
근데 저희 칩은 진짜 이만하거든요
여기 넣을 수가 없으니까 경량화를 해 보자
그러면 연산에 최적화된 NPU를 설계 해보자
이런식으로 연구를 진행했었습니다
그러다 보니까 이제 칩 연구가 너무 어렵다 졸업 못하겠다
이런 타이밍 와 가지고
칩은 이제 고정을 시켜 놓고 cpu gpu 많이 쓰잖아요
거기서 최적화를 한번 해보자
그런 식으로 생각을 좀 바꿔 가지고
그런 연구를 계속해서 이제 졸업했습니다
하드웨어 연구를 하신게 신기해요
NPU는 뉴럴 프로세싱 유닛이라고 대표적인게
이제 구글이 만든 tpu 같은 거 있잖아요
그래서 이제 뉴럴 연산에 최적화된 칩인데
그 하드웨어를 만드는 연구를 했다는 게 너무 신기하네요
4년차 정도까지 그런 NPU 연구를 진행했었고
하드웨어 지원이 있을 때
어떻게 하면
강력한 성능을 보이는 경량화 모델을 지원할 수 있을까
이런 방면에서도 연구를 하고 반면에 경량화를 했을 때
어떻게 하면 잘 지원하는 하드웨어를 구상 할 수 있을까
소프트웨어 방향이랑 하드웨어 방향에서 같이 연구
진행했었습니다 그래서 CPU나 GPU칩에서 모델이 어떻게 연산을 더
빠르게 할 수 있는 지 이런 것을 연구했다고 이해해도 되나요
어떻게 하드웨어가 주어졌을 때, CPU면 CPU, GPU면 GPU
이렇게 정해졌을때
최적의 연산 해주는 코드로 바꿀 수 있을지
이런 연구를 했다고 보시면 될 것 같아요
진님도 연구하실 때 쓰시겠지만
일반적인 연산들은 되게 잘 돼 있잖아요
파이토치 텐서플로우
이럴 것을 써보면 GPU도 잘되어있고 그 쪽에서 연구를 하면 사실
만들게 많지는 않기 때문에
저같은 경우에는 경량화를 진짜 엄청 많이 극한까지 한 모델을
어떻게 하면 CPU
GPU에서 잘 돌릴 것이냐 이런 방향으로 연구를 했었습니다
아직 CPU GPU 잘 되어 있지만 극한 경량화 모델은
지원 조금 덜 되거든요 하드웨어까지 같이 듣는
이런 AI 분야의 얘기는 진짜 듣기 힘든 거 같아요
제가 모델 학습
시키는 사람 입장에서 애한테 어떤 데이터를 주고
어떻게 뇌의 뉴런들을 조작해서 똑똑하게 만들까를 연구했다면
태수님은 뇌 구조 자체를 바꿔버리는 연구를 하신 것 같아서
너무 신기합니다
이렇게 대학원까지 가시다가
어떻게 창업을 하시게 됐어요?
이것도 사실 흔한 선택은 아니긴 하죠
이상한 선택들을 많이 하고 있는데
졸업하고 나서 동기들이랑 바로 창업을 한 케이스고 얼마 안 됐어요
작년에 창업을 했는데
경량화 자체의 특성 때문인 것 같은데요
소프트웨어 측면에서 이해도 되게 중요하고
하드웨어 측면에서 이해도 되게 중요한 그래서 그것을 다 해야 되는데
저도 이제 사람이니까 다 해결할 수는 없단 말이죠
그럴 거면 팀을 만들어야 된다
내가 못하는 것을 채워줄 사람이 필요하다
이런 생각을 했는데 어차피 랩에서는 하고 있었는데
그거 그냥 그대로 하면 되는 거 아닌가
이렇게 생각이 좀 흘러가서 창업한다는 것 자체가 어쨌든
리스크가 있는 선택니까
해도 되는가?에 대한 고민은 조금 있긴 했어요
3~4년 전 만해도 학회가서 모 회사 분들 만나서
어느 순간부턴가 '저 경량화 연구하고 있어요'하면
그러면 관심도 되게 많으시고 '저희 이런 서비스 해야 되는데'
혹시 이런 거 경량화 가능하냐
이런 식으로 계속 좀 물어보고
저희도 물어볼 거 물어보고
이런 순환이 가능해지더라구요
어쩌면 경량화 좀 핫한 분야 일지도? 이런 생각이 들면서
고민을 조금 덜었던 것 같습니다
본격적으로 경량화에 대해서 묻고 싶어요
제가 모델을 다이어트 한다고 계속 했는데
실제 용어는 경량화 MODEL COMPRESSION이라고 하죠
그래서 AI 경량화는 무엇인가요
기술적인 얘기를 좀 해야 될 거 같은데
경량화가 왜 필요해졌느냐를 아는 게
조금 더 필요할 것 같거든요
그래서 그런 백그라운드를 조금 설명을 드리자면
최근에 몇년만에
AI 모델 성능이 되게 많이 올라갔잖아요
GPT3라던지 큰 대규모 언어 모델들 LLM이라고 하는데
성능이 굉장히 좋아졌고
특히 ChatGPT가 성능이 진짜 좋았잖아요
그래서 인상적으로 하면서
대중들에게도 AI모델이 좋다는 게 널리 알려지고 있는 거 같아요
그거 에다가
요즘 Diffusion 모델 기반 서비스도 되게 핫하잖아요
LENSA라는 앱 같은 경우에
아바타 만들어 주는 서비스로 돈 되게 많이 벌었고
그런 서비스
몇 개가 있기는 한데 우리 실생활에서 쓰이고 있는
AI 기반 서비스가 많지 않다는 거를
또 사람들이 많이 느끼고 있는 거 같아요
저희 요걸로 서비스예요 하는 경우는 ChatGPT
아니면 LENSA 요즘은 SNOW에도 있는데
끝이잖아요 그런 문제가 있는 상황입니다
공감하는 게 ChatGPT가 엄청난 사건이었잖아요
성능이 너무 뛰어나서
근데 사실 사용하려면
OpenAI 홈페이지 들어가 거기서만 입력해야 되죠
실제로 우리가 생활하면서
우리가 쓰는 기능에 들어오지는 아직 안았어요
그래서 모델의 엄청난 성능과 그거에 적용간에
엄청 큰 간극이 아직 있는 거 같아요
네 실제로 일상적으로 우리가 쓰는 휴대폰이나 스피커에
아직 엄청 좋은 AI 모델이
들어가 있는 경우는 잘 없죠
이런 현상이 왜 그러냐를 생각을 할 수 있는데
이유는 되게 많은 거 같아요 AI모델의 성능 자체나
아니면 안전성 이슈 같은 것도 있을 수 있죠
원하지 않는 결과가 나온다든지 이상한 말을 한다든지
이럴 수 있으니까
그런 문제도 있을 수 있고
뭐 데이터 수집 문제
이런 것도 요즘 많이 핫하고요
근데 이런 것 중에 가장
중요한 문제가 있는데 바로 AI 모델의 서빙 비용
이 문제가 큽니다 어쩐지 LENSA를
제가 써봤거든요
아바타 만들어주는 근데 공짜가 아닌 거예요 셀카 20장 찍으라고 해서
20장 찍었더니 8천원 내라고
유료화가 될 수밖에 없는 게 조금 이유가 있기는 합니다
유저 입장에서
당연히 무료가 좋지만 서비스를 제공하는 입장에서 생각해보면
AI 모델은 연산이 엄청 많이 필요해요
그래서 Stable Diffusion
같은 것 요즘 써보기 되게 쉽게 되어 있는데
한번 돌려 보면은
집의 PC에서 한번 둘러 본 적이 있거든요 그럼 GPU가 엄청 앓는 소리를 해요
그런 것을 보면 연산이 진짜 많이 필요하구나
이렇게 느낄 수 있거든요
근데 이거를
그러면 이제 회사에서 AI로 서비스를 한다고 보면
결국엔 GPU를 엄청 많이 써야된다 그런 상황이 되거든요
그러다 보면 이제 회사들이 어떻게 해야 되느냐 서버를 빌려야 합니다
주로 AWS, 아마존 것, 아니면 KT 클라우드, 네이버 클라우드
이런 업체들에서 GPU서버, CPU서버 빌려서 쓰게 되는데
빌려서 쓰는 게 시간당 돈이 들다 보니까 결국에 돌린다는
행위 자체가 코스트가 되는거에요 원가처럼 되는데
이게 요즘 진짜 비싸거든요
전기를 많이 쓰는 이유도 있고
사람들이 GPU를 너무 많이 쓰다 보니까
비용 자체가 오른 요인도 있어서
그런 문제가 있고요
그래서 뭐 예를 들어서 진짜 숫자로 얘기해 보면
ChatGPT가 요즘 서비스를 하고 있는데
어떤 교수님이 예측한 바로는 정확한 숫자는 아닌데
서비스에 한 10만 달러 대충 1.2억 정도가 타고 있다
이런 얘기를 하신 경우가 있었거든요
비용 엄청 크다 GPT가 사실은 엄청 큰 모델이잖아요
1억 2천 이라는 단어를 들으니까 얼마나 큰지 알겠네요
OpenAI CEO가 트위터에서 그 얘기를 했었어요
Eye-watering하다 그 비용이 눈물이 난다
이렇게 얘기를 했었는데
제 생각엔 원래 돈 벌려고 나오지 않았을 것 같은데
결국에는 유료화가 도입되더라구요
그 이유가 서빙 비용 때문이라고 보시면 되는데
경량화는 그런 문제를 해결하는데 초점을 맞추고 있습니다
모델을 작거나
가볍게 만들어서
더 적은 비용으로 같은 서비스를 할 수 있게 만들어주는 것이
경량화라고 보시면 되고요
예를 들어서 서비스할 AI 모델을
경량화를 통해서 GPU를 4분의 1만 써도 되게 만들었다
이러면은 비용이 4분의 1로 딱 떨어지겠죠
이런 부분에서 임팩트가 있는 기술이라고 보면 되고 좀 더 공격적으로
경량화를 한다고 생각하면 서버를 쓰지 않고
그런 AI모델을
휴대폰이나 PC(여기서는 IoT 기기를 의미) 이런
작은 디바이스에서 올릴 수 있게 만드는 것도
또 하나의 접근 방법이라고 보시면 될 거 같습니다
그러면 서버를 안 쓰니까
각자 유저가 자기 디바이스에서 연산을 할 수 있으니까
그럼 비용이 아예 사라지겠죠
그래서 그런 거는
이제 공격적으로 경량화를 하는 부분이다라고 보시면 될 거 같고요
저희가 하는 일은 뭐냐 경량화를 통해서 모델을 줄이고
연산을 적게해서 코스트를 줄이는데
그 와중에 어떻게 하면 모델의 성능을 유지할 수 있겠느냐
이런 것을 연구하고 있다라고 보시면 거 같아요
성능을 유지하면서
연산을 줄여서
코스트를 세이빙 할 수 있는
분야 그게 경량화다 라고 보시면 될 것 같습니다
OpenAI 같이 거대 모델을 갖고 있는 회사들에게는
경량화 이콜 돈일 것 같아요
사장님들이
굉장히 좋아할 만한 영역인 거 같아요
그래서 들으면 들을수록
ChatGPT에 놀랄 게 아니라
여기만 주목할 게 아니라 과연 이것을 서빙 하는데
이런 경량화 같은 기술에 대해 관심을 가져야 한다고 생각하고
인공지능의 블루오션이란 생각이 너무 들어요
또 IoT나
핸드폰이나 작은
기기에서 서비스할 수 있다는 것도 말씀을 해주셨어요
현업에서는 두 가지 방법이 있을 것 같아요
서버를 사용하거나 아니면 디바이스 자체에 모델을 넣거나
두가지 경우에서 경량화가 어떤 식으로 진행되는지
궁금해요 경량화를 저희가 다룰 때
그렇게 현업에서 두 가지로 크게 나누고 있습니다
첫 번째가 모바일 기기
혹인 엣지(Edge) 기기에서
진짜 작은 모델을 바로바로 돌릴 수 있는 환경인데요
이런 서비스를 구성할 때
보면 보통 이제
실시간 서비스를 만드는 게
되게 중요한 제약(constraint)이 되거든요
예를 들어서
제 생각에는 인스타그램 앱 정도 볼 수 있을 거 같아은데
요즘 인스타그램 스토리에 보면
얼굴에 필터 씌우는 게 되게 많이 사용 되잖아요
근데 사실 또 AI 모델로 내 얼굴에 특징점을 잡아가지고
거기에다가 뭔가를 입히는 기술인데
그거를 필터링 씌운다고 생각했을 때 제가 하고 있는데
제가 이렇게 움직였는데 1초 늦게 따라간다
이상하잖아요
유저 경험(UX)이 되게 안 좋아질 텐데
경량화를 통해서 어떻게 하냐면
성능을 유지하면서 빠르게 작동할 수 있게 해서
유저 경험을 개선 하는데
집중을 한다고 보시면 될 것 같구요
같은 기능을 유지하면서 연산을 빠르게 끝내고
동시에 빠르게 끝나다 보니까 배터리도 적게 먹고
아니면 열도 적게 나게 한다
이런 게 주 목표로 보시면 될 거 같고요
해외 유망한 기업이 있는데
이 팀이 초창기에 했던 일 중에 하나가 아마존에
에코 스피커가 있었어요
그래서 에코를 보면
보면은 Hey Alexa 이런 기능이 있는데 응답 지연 시간(latency)가 워낙 중요하다 보니
제가 Hey Alexa 했는데 1초 있다가 네 대답하면 좀 이상하잖아요
이 것을 경량화해서 응답 지연 시간(latency)를 줄여서
에코 스피커에 올라갔었다 이런 연구를 많이 했었거든요
그래서 실제로
이렇게 적용되는 케이스가 있었다고 보시면 되고요
그외에 모바일 환경에서
저희 회사에서 하고 있는 일들이 되게 많은데
안타깝게도
아직 공개할 수 없는 부분들이 많아 가지고
엣지 디바이스, 온 디바이스 정말 많이 들어 봤거든요
엣지 컴퓨팅
이런 것들이 IoT 자체에
인공지능 모델 탑재를 해서
서버에서 오는 시간을 줄이는 거 잖아요
아까 비용을 얘기 하셨지만
엣지 디바이스 같은 경우에는 경량화가 유저 경험과도
밀접하게 연결되어있는 것 같아요
제가 알렉사에게
이야기한 것들이
이제 서버에 간다고 생각하면은 약간 찜찜 하거든요
근데 데이터가 서버까지 안 가고
내 디바이스 끝낼 수 있다
데이터 안정성에 대한 장정도 있을 것 같아요
여기까지가 엣지 디바이스 분야고
다른 큰 가지 중에 하나는 말씀드렸듯이
서버에서 계산을 할 수 있다
이런 분야인데요
서버 기반 AI 서비스 같은 경우에
비용에 영향을 되게 많이 받는데
그거를 해결하기 위해서
경량화가 많이 적용되고 있는 상황이에요
서버 비용을 줄이는 데 많이 적용이 되고 있고
또 다른 문제로 들 수 있는 게
이젠 워낙 환경 문제에 사람들이 관심이 많잖아요
AI모델이라는 게 돌려 보면 아시겠지만
전기를 진짜 많이 쓰거든요
전기 먹는 하마 인데
그 말인즉슨
전기를 만들기 위해 쓰는 탄소 발자국이 엄청 크다
이렇게 볼 수도 있겠죠
경량화를 하면은 이런 부분이 많이 해결이 될 수 있다
저희는 이렇게 또 말씀을 드리고 있습니다
그래서 수치가 아니면 또 와 닿지 않는 부분이 있는데
이것도 사실 추상치인데요
학습(training) 한번 하는데 이산화탄소가 552톤 정도 나온다
근데 알려져있기로
차 한대가 1년에 한 3톤 정도 나온대요
그러면 대충 예상했을 때 180대가 1년 다닐만큼의
탄소가 ChatGPT 한 번 학습 하는데 든다는 거겠죠?
그런 데서 보면은 저희가 학습을 경량화 하고 있지 않았지만
경량화라는 게 진짜 환경적으로도 중요하다
볼 수 있을 것 같으니까 사회적으로 좋은 일 하시네요
그러면 지금 얘기해 주신 분야 말고도
진짜 경량화 꼭 써야 된다 이런 분야가 있을까요
어느 모델이든
어느 분이든
인공지능으로 서비스를 하려면 경량화가 필요한 거 같아요
연구에서 넘어가서 상용화 한다고 생각하면
서비스 비용(cost)나
유저 경험(UX)이 너무 중요하다 보니까 그런 거 같은데요
그래도 그 중에 하나를 뽑아보자 라고
하면은 첫 번째는 센서에
딱 붙어 있는 IoT 분야가 있을 거 같고
두번째는 초거대 모델들, 언어 모델 이런 것이 중요할 것 같습니다
센서 같은 경우에는 센서가 있을 때
거기에 IoT 디바이스 되게 작은 것들을 가깝게 배치하고
거기에는 작게 만든 경량화된
뉴럴 네트워크 모델을 돌리면서
연산을 할 수 있으면 통신(networking)도 되게 줄어들고
하다 보니까
좀 더 다양한 센서나
다양한 어플리케이션을 만들 수 있을 것 같다는 생각이 드는
것 같고요
대규모 언어 모델 같은 경우에는 말씀드렸지만
비용(cost) 이런 측면이 너무나도 중요하기 때문에
앞으로도 계속해서 경량화가
사용 되지 않을까라는 생각이 드는 거 같아요 요즘 AI 모델을
쓴다는 게 IoT 센서 방향이나
거대 모델 방향이 진짜  유명한 분야잖아요
여기에 쓰고 싶다
여기에 쓰고 싶다 엄청 많이 얘기하는데
거기에 저희가 딱 align이 잘 되어있다 보니까
경량화가 앞으로 AI 서비스를 만드는 측면에 있어서
계속해서 같이 가지 않을까
어떤 분야든 사용 되지 않을까 라고 생각을 합니다
AI, 인공지능 분야의 블루 오션 이라고
팩트 체크를 딱 하고 가는 것 같아요