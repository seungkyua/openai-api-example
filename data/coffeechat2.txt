그러니까 경량화가 왜 필요한지
너무 설명 잘 해주신 것 같아요
사실 구글링 해보면 왜 필요한지는 나오는데
어떻게 하는지 how에 대한 설명을 진짜 없거든요
그래서 제가 오늘 커피챗
신청드리면서 이 부분을 알고 싶어서
신청 드렸어요
대표적인 경량화
방법론이 있다면 좀 소개를 해 주실 수 있나요
이렇게 네 가지를 저희가 보통 얘기를 드리고 있어요
그래서 Quantization이나 Pruning
같은 경우에는 모델이 있을 때
연산을 어떤 방식으로 할 거냐
이거를 저희가 손을 대서
경량화를 하는 방식이라고 보시면 될 것 같고요
반면에 Knowledge Distillation이나 AutoML
같은 경우에는 모델을 구조 자체를 어떻게 바꿀 것이냐
그래서 어떻게 바꿔서 성능을 유지하면서
경량화를 할 것이냐
이렇게 다루는 방법 이라고 보시면 되겠습니다
그래서 네 개가 사실 서로 막 배타적인 것은
아니에요 Quantization했다고 Pruning 안 되냐
이런 건 아니고
취사선택해서 상황에 따라서 최대한 좋은 방식을 찾는다
이렇게 보시면 될 것 같아요
연산이랑 구조 단어가 어려워요
혹시 사람이 다이어트 한다고 생각해 봐서
거기다 비유를 좀 해주실 수 있나요
요런 거가 이제 연구하는 사람들
되게 어려워하는 것 같아요
사람의 다이어트 다라고 생각을 해보면
Quantization이나 Pruning 같은 경우에는
실제로 뭐 살을 빼는 거다
이렇게 보시면 될 것 같은데요
일반적으로 몸에서 지방을 줄이거나
아니면 하다 하다 보니까 안되겠어요
그러면 머리카락을 자르잖아요
머리를 밀어버리는
그렇게라도 해서 몸무게를 줄이는 방식이다
반면에 Knowledge Distillation이나
AutoML 같은 경우에는
같은 일을 좀 낮은 체급 낮은 몸무게로도
어떻게 하면
잘할 수 있을까를 보는 방향이라고 보시면 될 것 같아요
복싱을 한다 헤비급 선수 되게 대단한 선수가 있어요
그 사람이 훈련하는 루틴을 막 보다가 이거 좋네
이거 좋네 해가지고 속속 뽑아 가지고 좀 페더급 선수
아무래도 훈련을 그만큼은 못할 테니까
이렇게 딱 좋은 것만 적용해 가지고
되게 잘하는 사람을 만드는 방법이 뭐냐
이렇게 연구하는 거라고 보실 수 있을 것 같고
AutoML같은 경우에는 반면에 그런 방법
찾아줘 누구한테 찾아줘 이렇게 부탁하는 것
그런 모델을 만드는 거라고 보시면 될 것 같습니다
이렇게 비유하니까
너무 이해가 잘돼요.
대표적인 방법론들에 대해서 자세하게 설명을 부탁드려도 될까요?
Quantization부터 하자면
이제 모델에서 사용되는
여러 파라미터들의 precision이라는 게 있거든요
그 precision을 줄이는 기술입니다
컴퓨터에서 이제 숫자 하나를 얼마나 정밀하게 표현할 것이냐
이거를 precision 이라고 저희가 표현을 하는데
일반적으로 뉴럴 네트워크에 써있는 게
이제 32비트
실수 플로팅 32비트 이렇게 얘기를 많이 하는데요
Quantization은 이 숫자들을 좀 더 짝게 플로팅 포인트 십육
비트 짜리 십육 비트로 표현되는 실수나
아니면 더 짝게 내려가면 8비트 integer
정수 혹은 4비트
정수 진짜 작게는 일
비트 정수까지 내려서
그렇게 연산을 하는 방식이라고 보시면 될 것 같습니다
이같은 경우에는
아무래도 표현할 수 있는 범위가 작아지니까
정확도가 좀 많이 떨어지는 측면이 있거든요
그래서 어떻게 하면 정확도 유지하면서
작게 만들 것이냐를 연구하는 분야인 것 같아요
이제 모델 같은 경우에는 인풋이나 weight에 무수
하게 많은 숫자들이 저장되어 있잖아요
이것들의 precision을 변환해서
메모리를 줄인다 이 말씀이시죠
회사명 Squeeze Bit가 이 비트를 말하는 건가
그것도 맞습니다
원래 모델에서도 인풋이나 weight에 숫자가 워낙 많다 보니까
그거에 비트 수를 줄여서 최대한 경량화를 하는 건데
저희는 짓고 나서 되게 뿌듯했거든요
이걸 잘 지었다 대박이다 이렇게 얘기했었는데
사실 잘 못 알아들으세요
Pruning에 대한 설명도 해주실 수 있나요?
Pruning 같은 경우에는
이렇게 전체적으로 precision을 줄이는 방식이 아니고
모델에서 덜 중요한 파라미터, 인풋이나 weight가 있을 때
개를 아예 잘라 내는 방식이
그래서 Pruning 영어 단어
뜻 자체가 가지치기 잖아요
그래가지고 중요하지 않은 애들을 잘라 내는 방식인 건데
모델에서도 워낙 파라미터가 많다 보니까
학습을 끝내고 나면
좀 전체 성능에
별로 영향이 없는 파라미터들이 생기게 되거든요
그래서 그걸 어떻게든 잘 찾아가지고
그걸 0으로 날려 버린다든지
이런 식으로 하는
가지치기 방식이라고 보시면 될 것 같아요
그래서 파라미터를 많이 날리면
날릴수록 자르면 짧을수록 당연히 모델은 작아지는데요
대신 이것도 마찬가지로
너무 많이 자르면 전체 모델의 성능이 떨어진다
이렇게 보시면 될 것 같습니다
연결해서 이제 Knowledge Distillation 설명도 같이 해드리면
Knowledge Distillation은 크고 성능이 되게 좋은 모델이 있을 때
얘가 뭔가를 배웠을 거잖아요
그 지식을 가지고
이제 더 작은
모델 같은 일을 하는 모델을 학습시키는 방법
이라고 보시면 될 것 같아요
그래서 Distillation 이라는게 증류잖아요
뭔가 있는 걸 응축
시켜서 더 응축된 것을 만든다 라는 뜻인데
그래서 이런 큰 모델의 지식을 증류해서
작은 모델이 지식을 습득하게 한다
라는 측면에서 이런 이름이 붙었고요
그래서 같은 구조의 모델이어도 Knowledge Distillation을 적용하면
성능이 훨씬 좋은 경우가 되게 많아요
그래서 뭐 앞에서 했던 Quantization이나
Pruning이랑은
다르게 모델의
연산방식 자체를 손대는 것이
아니고 이 구조가 작아졌을 때
얘를 얼마나 어떻게 하면 잘 학습시킬 수 있을까
그래서 이 학습방법을
어떻게 찾는지에 대한 방법론이라고
보시면 될 것 같습니다
아까 4가지 말씀드렸었는데
마지막으로 AutoML이 있었잖아요
이 경우에는
모델의 구조를 좀 자동으로 찾아낼 수 있도록
하는 기술이에요
그래서 이거 같은 경우에는
오토라는 말에서도 볼 수 있듯이
저희가 일일이 손으로 다 테스트해보고
이게 더 작은데 성능이 더 좋다
이걸 하는 게 아니고 뭔가 이런 모델에 맡긴다든지
아니면 룰을 만들어서 테스트를 하게 해서
최종적으로는 자동으로
최적의 구조가 나오도록 하는
이런 방향이고요 이 AutoML 분야는 이것만 해도
엄청 큰 분야에 요
경량화 뿐만이 아니라
더 성능이 좋은 모델을 찾으려고
많이 사용되고 있는 분야여서
이걸 전부 설명하게 하는 시간이
너무 많이 될 것 같아서 이 정도로
개념적인 설명만 드리면 될 것 같습니다
심화 질문을 드리고 싶은데
먼저 Quantization에서 모델을 학습할 때 사용하는 건지
아니면 추론할 때 사용하는 건지 궁금해요
지금까지 계속 설명드린거는 추론에
좀 더 비중이 높다고 보시면 될 것 같아요
가장 중요한 분야가 추론이기 때문이기도 하고요
그 이유는 training 할 때
드는 코스트는 사실
training 한번 끝내고 나면
그거를 이제 서비스에서 엄청
여러번 추론을 할 수 있잖아요
그러다 보니까
training cost는 이 여러 개의 추론에
좀 약간 할부처럼 갚을 수 있는 코스트다
이렇게 생각하시는 분들이 많아가지고
아무래도 조금 더 덜 연구되는 부분이 있기는 하고요
그래도 요즘 모델이 워낙 커지다 보니까
학습 코스트를 무시할 수가 없어서
여기에도 Quantization을 적용하려는 시도가 많은데요
예를 들어서 gradient를 Quantization 한다든지
학습과정에서 그런 일들이 좀 진행이 되고 있습니다
Pytorch에 AMP라는 기능이 있어요 Automatic Mixed Precision이라는 기능이 있는데
이거는 이제 Nvidia GPT에서 16
비트 플로팅 포인트를 지원을 하거든요
16 비트 실수 연산을 지원을 하는데
이걸 가지고
학습을 시킬 수 있도록 지원을 하는 기능이 있고요
그래서 실제로 flag 한 번만 키면은 쓸 수 있게 돼서
되게 편하게 는 되어 있어요
그래서 이걸 써보면
이제 학습도 좀 빨라지는 것을 확인을 할 수 있고요
근데 문제가 뭐냐면
아무래도 32
비트에서 16 비트로 표현가능성이 줄어들다보니까
그래디언트가 약간 값이 이상해져서
모델이 converge되지 않는 경우도 꽤 많이 보여지기는 합니다
그래서 아무래도 되게 좋고
쓸 수 있으면 되게 좋은 기능인데
가끔 문제가 생기기 때문에 좀 주의해서 써야 된다
이렇게 보시면 될 것 같아요
또 Quantization이 연산과 관련된 부분이기 때문에
하드웨어와 되게 밀접하게 연결이 될 것 같아요
내가 어떤 칩을 쓰느냐에 따라서 방법론도 달라지나요?
이거는 얘기할 때 되게 많은데요
제가 박사를 이걸로 하기 때문에 맞다
하드웨어 종류에 따라서
지원하는 연산의 종류가 되게 많이 바뀌거든요
예를 들어서 GPU같은 경우에는 floating point 32 16
혹은 요즘에는 bloat 16 이라고 좀 특이한 포맷도 있고요
그리고 cpu같은 경우에 int 32 8 내려가면서
어떤 걸 지원하느냐가 이제 아키텍처별로 또 갈리기
이기 때문에 우리가 타겟하고 있는 아키텍처가 뭐냐
에 따라서 Quantization쓸 수 있는 게 딱 정해진다라고 보시면 될 것 같아요
GPU같은 경우에 보면은 tensor core라는 게 있거든요
이런 되게 빨라지는 하드웨어가 들어있는데
이것도 아키텍처벌로 되게
다른 게 암페어 아키텍처까지는
INT 4bit 연산이 지원이 됐었어요
그런데 이번에 나온 호퍼 아키텍처에서는 이게 빠졌거든요
그래서 이렇게 세대별로 조금씩 차이가 있고
잘 알고 선택을 해야 된다라고 보시면 될 것 같습니다
마지막으로 Quantization 방법론 장점은 뭔가요
어떤 경우든 Quantization 적용하면은 모델 크기랑
연산 속도 둘다 개선할 수 있다는 점인 것 같아요
거의 대부분의 하드웨어들이 뭐 8bit
정수 연산자는 보통 가지고 있고요
요즘 같은 경우는 16 bit
실수 연산기들도 또 많이 들어있는 경우가 있어서
이런 게 지원이 많이 되고 있다 보니까
좀 어떤 하드웨어도
Quantization을 특정 수준까지는 할 수 있다
이런 게 되게 장점인 것 같아요